<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Parallel Mesh Downsampling</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
      <h1>Parallel Mesh Downsampling</h1>
        <p>
          Aaron Perley (aperley)    <br />
          Siddhant Madhuk (smadhuk)
        </p>
        <img src="ball.PNG" width="60%" style="margin: 0 auto;"></img>
        <img src="ball_downsampled.PNG" width="60%" style="margin: 0 auto;"></img>
      </header>
      <section>
        <h3>Checkpoint (4/19)</h3>
        <ul>
          <li>
            We managed to get OpenMP working with Aaron's mesh downsampling code from
            15-462. As a test, we used OpenMP to parallelize the trivially parallelizable parts
            of the computation, which are just <code>map</code>s over the edges and vertices in the mesh,
            and saw an expected speedup.
          </li>
          <li>
            We have begun our initial parallel attempt, which attaches mutexes to every edge and vertex in the
            mesh and we have modified the mutable priority queue to be thread safe. The bulk of our work in the next
            week will be completing and polishing the thread safe halfedge mesh. We have also investigated using
            SIMD in addition to multithreaded parallelism, since many parts of the code look like they will work with SIMD.
          </li>
          <li>
            We will be presenting graphs of our speedup, as well as videos of the difference in the speedup of downsampling using
            serial vs. parallel implementations. The videos will allow us to compare not only the time difference, but also any changes
            in the quality of the downsampling.
          </li>
        </ul>
        
        Revised Schedule
        <ul>
          <li>
            <strong>4/20 - 4/24:</strong> Aaron: Finish fine-grained locking implementation of halfedge mesh.
            Siddhant: Modify mutable priority queue to use more fine grained locking.
          </li>
          <li>
            <strong>4/25 - 4/27</strong> Aaron: Get code working on the Xeon Phis.
            Siddhant: Begin working on spatial partitioning scheme.
          </li>
          <li>
            <strong>4/28 - 5/1</strong> Aaron: Investigate and implement the use of atomics for ensuring the halfedge
            mesh is thread safe. Aaron and Siddhant: Finish implementing spatial partioning scheme.
          </li>
          <li>
            <strong>5/2 - 5/5</strong> Aaron: Implement the system using CUDA, if feasible.
            Siddhant: Tweak data structures and algorithms to take advantage of spatial partitioning scheme.
          </li>
          <li>
            <strong>5/6 - 5/9</strong> Aaron and Siddhant: Write up results, prepare graphs and videos.
          </li>
        </ul>
        
        
      </section>
      
      
      <section>
        <h3>Summary</h3>
<p>We are going to investigate the performance of different implementations of mesh downsampling via quadratic error simplification on multicore CPUs. We will focus on ways to efficiently collapse edges in parallel, while minimizing synchronization necessary to maintain the correctness of the data structure, and producting a high quality downsampling.</p>

<h3>Background</h3>
<p>
  Halfedge triangle meshes are used in computer graphics to represent surfaces in three dimensions. When a mesh has more vertices and edges than are necessary to effectively model a shape, downsampling can be used to collapse unnecessary edges. Although it is not possible to determine a completely optimized edge collapse order, the quadric error metric developed by Michael Garland and Paul Heckbert provides a heuristic to determine good edge collapse candidates. The basic algorithm computes a quadric error score for each edge and then uses a priority queue to iteratively collapse the edges with the best score. After each edge is collapsed, other edges may also be removed and the heuristic score for some edges may need to be updated. This introduces many opportunities for parallelism. Trivially, we can parallelize the computation of the quadric error scores for each edge. In addition, we will also collapse edges and update quadric error scores in parallel. This will require synchronization between threads to maintain correctness and provides an opportunity to test different work partitioning schemes.
</p>

<h3>The Challenge</h3>
<p>
  This problem is challenging since removing edges in a halfedge mesh in parallel requires synchronization so that a thread is not trying to traverse or remove edges that another thread has removed or edited. We will investigate the performance of different synchronization techniques including coarse-grained locks on the entire data structure, fine-grained locks on individual mesh elements, transactional memory operations, as well as lock free solutions that ensure that conflicting updates never occur. We will also investiage different work and spacial partitioning schemes, to ensure that synchronization between threads is relatively infrequent. Finally, traversing the halfedge data structure can have very incoherent memory access.
</p>

<h3>Resources</h3>
<p>The starter code that we will be working with will be from assignment 2 from 15-462, MeshEdit. The code contains a serial solution to the probelem that we are trying to tackle and we will be using it as a reference while writing our parallel code. We will start by working on the six-core Xeon e5 in the lateday cluster initially and also the Xeon Phi to see which gives us better performance. We will be looking at the difference in performance obtained when we use different methods of synchronizations across the two machines and as we use different implementations for our code. Then we will investigate the performance of our code when it has been ported to the CUDA language so it can work on NVIDIA GPUs.</p>

<h3>Goals and Deliverables</h3>
<p>
  Our goals for the project are:
  <ul>
    <li>Implement a working serial version, since our starter code still has a few glitches that we need to resolve.</li>
    <li>Implement a naive parallel version by basically locking all the edges in our mesh whenever an operation is being performed on them.</li>
    <li>Investigate the effects of using different parallel approaches, such as transactional memory, coarse grained locks and fine grained locks on mesh elements.</li>
    <li>Investigate the possibility of using different data structures to implement out mesh. Currently we are building half edges out of a given input which leads to a problem where edges that are close to each other on the mesh are actually at very different memory addresses, leading to incoherency issues. We will try and find a way to avoid this issue.</li>
    <li>Our first implementation will be designed to work on the lateday CPUs, we will also see the difference in the speedup achieved on the six core Xeon e5 and the fifty core Xeon Phi based on different approaches.</li>
    <li>The next step will be to port the code to work on GPUs in the Gates clusters and investigate the performance of our code on these machines.</li>
  </ul>
</p>

<h3>Platform Choice</h3>
<p>Our problem ends up being IO-bound rather than bound computationally. Since problems that are bound by IO are usually better suited for CPU tasks, we will be implementing our code on the latedays cluster first. We will then later check the performance of our code on the GPUs of the Gates, but this will be a secondary objective in nature.</p>

<h3>Schedule</h3>
<p>
  <ul>
    <li><strong>Friday, April 8:</strong> Finish getting correct serial version working on Latedays. Finish simple parallel version with a lock on the entire data structure.</li>
    <li><strong>Friday, April 15:</strong> Finish fine-grained locking version and comparisons to serial version running on Xeon e5-2620s and Xeon Phis.</li>
    <li><strong>Friday, April 22:</strong> Finish implementation of spatial partitioning scheme and comparisons to other versions. Also test tweaks to the data structure to improve memory accesses.</li>
    <li><strong>Friday, April 29:</strong> Port code to CUDA and test on the Gates 5000 machines.</li>
    <li><strong>Friday, May 6:</strong> Prepare visualisations of results and demo.</li>
  </ul>
  
</p>

</section>
      <footer>
        <p><small>Images from <a href="http://462cmu.github.io/asst2_meshedit/">15462 Assignment 2</a></small></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
